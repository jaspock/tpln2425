# Architectures for written-text processing

En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio P√©rez Ortiz. El bloque comienza con un repaso del funcionamiento del regresor log√≠stico, que nos servir√° para asentar los conocimientos necesarios para entender posteriores modelos. A continuaci√≥n se estudia con cierto nivel de detalle *skip-grams*, uno de los algoritmos para la obtenci√≥n de *embeddings* incontextuales de palabras. Despu√©s se repasa el funcionamiento de las arquitecturas neuronales *feedforward* y se estudia su aplicaci√≥n a modelos de lengua. El objetivo √∫ltimo es abordar el estudio de la arquitectura m√°s importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un an√°lisis del funcionamiento de los modelos preentrenados (modelos fundacionales), en general, y de los modelos de lengua, en particular.

Los materiales de clase complementan la lectura de algunos cap√≠tulos de un libro de texto ("Speech and Language Processing" de Dan Jurafsky y James H. Martin, borrador de la tercera edici√≥n, disponible online) con anotaciones realizadas por el profesor.

## Primera sesi√≥n de este bloque (20 de diciembre de 2023)

### Contenidos a preparar antes de la sesi√≥n del 20/12/2023

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de los contenidos de [esta p√°gina](https://dlsi.ua.es/~japerez/materials/transformers/regresor/) sobre regresi√≥n log√≠stica. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop√≥sito es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo del libro. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.
- Visionado y estudio de los tutoriales en v√≠deo de esta [playlist oficial de PyTorch](https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN).  Estudia al menos los 4 primeros v√≠deos (‚ÄúIntroduction to PyTorch‚Äù, ‚ÄúIntroduction to PyTorch Tensors‚Äù, ‚ÄúThe Fundamentals of Autograd‚Äù y ‚ÄúBuilding Models with PyTorch‚Äù). En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo.
- Tras acabar con las dos partes anteriores, realiza este [test de evaluaci√≥n](https://forms.gle/V3U9MTHo7c9DNhkc6) de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.

### Contenidos para la sesi√≥n presencial del 20/12/2023

En la clase presencial (2,5 horas üïíÔ∏è de duraci√≥n), veremos c√≥mo se implementa un regresor log√≠stico en PyTorch siguiendo las implementaciones de un regresor log√≠stico binario <a href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/logistic.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab\"></a> y de uno multinomial <a href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/softmax.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab\"></a> que se comentan en [este apartado](https://dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-un-regresor-logistico-y-uno-multinomial).

La idea es que vayas estudiando y modificando ligeramente los notebooks que vayamos estudiando. En una clase posterior se presentar√° una pr√°ctica m√°s avanzada que implicar√° modificar el c√≥digo del transformer.

## Segunda sesi√≥n (10 de enero de 2024)

### Contenidos a preparar antes de la sesi√≥n del 10/01/2024

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de los contenidos de [esta p√°gina](https://dlsi.ua.es/~japerez/materials/transformers/embeddings/) sobre los embeddings. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop√≥sito es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo del libro. En total, esta parte deber√≠a llevarte unas 4 horas üïíÔ∏è de trabajo.
- Lectura y estudio de los contenidos de [esta p√°gina](https://dlsi.ua.es/~japerez/materials/transformers/ffw/) sobre las redes neuronales hacia delante y su uso como modelos de lengua muy b√°sicos. Realiza al menos dos lecturas complementadas con las notas del profesor como en el punto anterior. En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo.
- Lectura y estudio de los contenidos de [esta p√°gina](https://dlsi.ua.es/~japerez/materials/transformers/attention/) de introducci√≥n a los transformers. Realiza, como siempre, al menos dos lecturas complementadas con las notas del profesor. En total, esta parte deber√≠a llevarte unas 4 horas üïíÔ∏è de trabajo.
- Tras acabar con las partes anteriores, realiza este [test de evaluaci√≥n](https://forms.gle/7KDwRtXcrpxsKjHp7) de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.
- Aprovecha, si te queda tiempo, para repasar los contenidos de la primera sesi√≥n.

### Contenidos para la sesi√≥n presencial del 10/01/2024

En la clase presencial (5 horas üïíÔ∏è de duraci√≥n), veremos c√≥mo se implementa en PyTorch el algoritmo de skip-grams <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/skipgram.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>, un modelo de lengua basado en red neuronal hacia delante <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/ffnn.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> y un transformer <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/transformer.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> siguiendo las implementaciones que se comentan en [este apartado](https://www.dlsi.ua.es/~japerez/materials/transformers/implementacion/#codigo-para-skip-grams) y los dos siguientes.

La idea es que vayas estudiando y modificando ligeramente los notebooks que vayamos estudiando. En una clase posterior se presentar√° una pr√°ctica m√°s avanzada que implicar√° modificar el c√≥digo del transformer.

## Tercera sesi√≥n (17 de enero de 2024)

### Contenidos a preparar antes de la sesi√≥n del 17/01/2024

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de los contenidos de [esta p√°gina](https://dlsi.ua.es/~japerez/materials/transformers/attention2/) sobre, por un lado, el modelo transformer completo (con codificador y descodificador) y, por otro, los posibles usos de una arquitectura que solo incluye el codificador. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. En particular, tendr√°s que leer algunas secciones del cap√≠tulo sobre traducci√≥n autom√°tica y otras del cap√≠tulo sobre modelos preentrenados, adem√°s de alguna secci√≥n suelta sobre *beam search* y tokenizaci√≥n en subpalabras. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop√≥sito es ayudarte a entender los conceptos clave de cada apartado. Despu√©s, realiza una segunda lectura de los contenidos del libro. En total, esta parte deber√≠a llevarte unas 4 horas üïíÔ∏è de trabajo.
- Visonado y estudio de la clase de Jesse Mu titulada "[Prompting, Reinforcement Learning from Human Feedback](https://youtu.be/SXpJ9EmG3s4?si=j4B1U2Z-JCyYJwlc)" del curso CS224N de Stanford de 2023 sobre modelos de lengua basados en el descodificador del transformer. En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo, porque tendr√°s que tomar notas del v√≠deo para no tener que verlo cada vez que quieras repasar algo; para tomar notas, te puede venir bien descargar las [diapositivas](https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture11-prompting-rlhf.pdf) y escribir sobre ellas. Puedes quedarte solo con las ideas b√°sicas de lo que se comenta entre los minutos 39 y 46, porque las ecuaciones del aprendizaje por refuerzo son un tema no prioritario para esta asignatura que ver√°s en otras asignaturas. Es importante que antes de ver el v√≠deo repases lo que ya estudiaste sobre los [transformers](https://dlsi.ua.es/~japerez/materials/transformers/attention/) como modelo de lengua basado en el descodificador. Que no te confunda que a los modelos basados en codificador tambi√©n se les conozca a veces con el nombre de modelos de lengua. En este v√≠deo se habla de las propiedades de modelos basados en descodificador que han sido entrenados para predecir el siguiente token de una secuencia.
- Estudia la descripci√≥n sobre [modelos multiling√ºes](https://dlsi.ua.es/~japerez/materials/transformers/attention2/#multilingual-models) que se hace en este apartado de una de las p√°ginas sobre transformers. Es un apartado breve que te llevar√° unos üïíÔ∏è 15 minutos.
- Tras acabar con las partes anteriores, realiza este [test de evaluaci√≥n](https://forms.gle/GRK5SLc3STkup8at9) de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.
- Aprovecha, si te queda tiempo, para repasar todos los contenidos de las sesiones anteriores.

### Contenidos para la sesi√≥n presencial del 17/01/2024

En la clase presencial (5 horas üïíÔ∏è de duraci√≥n), veremos c√≥mo se implementa sobre nuestro c√≥digo de la arquitectura transformer tanto un modelo de lengua basado en descodificador <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/lmgpt.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> como un modelo de reconocimiento de entidades nombradas <a target="_blank" href="https://colab.research.google.com/github/jaspock/me/blob/main/docs/materials/transformers/assets/notebooks/nerbert.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> basado en codificador. 
  
Aprovecharemos para repasar algunos aspectos del c√≥digo de sesiones anteriores y relacionar los aspectos te√≥ricos con los pr√°cticos. Presentaremos tambi√©n la pr√°ctica que tienes que entregar para este bloque de la asignatura.

## Cuarta sesi√≥n (19 de enero de 2024)

Esta cuarta sesi√≥n es realmente la primera y √∫nica sesi√≥n del tema de voz. Mira la p√°gina sobre [voz](speech.md) para ver los contenidos previos a esta sesi√≥n.
